import sys
sys.path.append("/home/sftpuser")
sys.path.append("/home/student/gitrepo/pyspark_ds/signalr")
from pyspark.sql import SparkSession
from signalr_client import send_progress, send_solution, send_update
import input_file
import atexit
from signalrcore.hub_connection_builder import HubConnectionBuilder
import threading
import time
from pyspark import AccumulatorParam

		#.config("spark.executor.instances", "1") \


def main():
	spark = SparkSession.builder \
		.appName("pbf") \
		.master("spark://172.16.3.2:7077") \
		.getOrCreate()
	sc = spark.sparkContext
	sc.addPyFile(f"/home/sftpuser/input_file.py")
	sc.setLogLevel("INFO")
	atexit.register(lambda: sc.stop())

	def stop_spark():
		#sc.stop()
		exit()

	class StringAccumulatorParam(AccumulatorParam):
		def zero(self, initialValue=""):
			return initialValue

		def addInPlace(self, v1, v2):
			return v1 + v2

	result_acc = sc.accumulator("", StringAccumulatorParam())
	progress_accumulator = sc.accumulator(0)
	switch = sc.accumulator(0)
	isFound = sc.broadcast(False)
	total_posibilities = len(input_file.input_data())
	data_rdd = spark.sparkContext.parallelize(input_file.input_data(), 100)


	def process_data(input_function, data_value, accumulator, switch, result_acc):
		if isFound.value:
			return None

		result = input_function(data_value)
		accumulator.add(1)

		if result[1]:
			result_acc.add(result[0])
			switch.add(1)

		return result


	def monitor_progress(isFound, switch, progress_accumulator, total_posibilities):
			start_timer = time.time()
		print("Setup complete, monitoring on")
		send_update(["Cluster innitialized, progress monitoring ON"])
		previous_percentage = 0
		while progress_accumulator.value <  total_posibilities and switch.value == 0:
			current_percentage = int(progress_accumulator.value / total_posibilities * 100)
			if current_percentage > previous_percentage:
				print(f"Progress: {current_percentage}%")
				send_progress([current_percentage, progress_accumulator.value])
				previous_percentage = current_percentage
				if switch.value > 0:
					break
			time.sleep(0.01)

		print(f"Success: {result_acc}")
		send_solution([result_acc.value, round(time.time() - start_timer, 3), progress_accumulator.value])
		stop_spark()
		#isFound.unpersist()
		#isFound = sc.broadcast(True)

	progress_thread = threading.Thread(target=monitor_progress, args=(isFound, switch, progress_accumulator, total_posibilities))
	progress_thread.start()

	result = data_rdd.map(lambda x: process_data(input_file.input_function, x, progress_accumulator, switch, result_acc)).filter(lambda x: x[1] == True).first()

	progress_thread.join()


if __name__ == '__main__':
	main()
